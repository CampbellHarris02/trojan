{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b501d36b",
   "metadata": {},
   "source": [
    "# TINY CLIP = fast but less accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60241935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/campbellharris/Desktop/trojan/trojan/python/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector shape: (512,), dtype: float32\n",
      "{'app': 'text_editor', 'action': 'coding', 'confidence': {'app': 0.28129708766937256, 'action_raw': 0.2698988914489746, 'action_boosted': 0.18177959322929382}}\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from PIL import Image\n",
    "from transformers import CLIPTokenizerFast\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 0) Load ONNX model session once\n",
    "img_session = ort.InferenceSession(\"tiny_clip/model.onnx\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1) Utilities\n",
    "def compress_image(img, quality=100):\n",
    "    buf = io.BytesIO()\n",
    "    img.save(buf, format=\"JPEG\", quality=quality)\n",
    "    return buf.getvalue()\n",
    "\n",
    "def embed(session: ort.InferenceSession, jpeg_bytes: bytes) -> np.ndarray:\n",
    "    \"\"\"JPEG bytes --> 512-D Tiny-CLIP vector.\"\"\"\n",
    "    img = (Image.open(io.BytesIO(jpeg_bytes))\n",
    "           .convert(\"RGB\")\n",
    "           .resize((224, 224), Image.Resampling.BICUBIC))\n",
    "\n",
    "    arr = (np.asarray(img, dtype=np.float32).transpose(2, 0, 1) / 127.5) - 1.0\n",
    "    arr = arr[np.newaxis, ...]  # (1,3,224,224)\n",
    "\n",
    "    ids = np.zeros((1, 77), dtype=np.int64)  # dummy\n",
    "    mask = np.ones((1, 77), dtype=np.int64)  # dummy\n",
    "    feeds = {}\n",
    "    for inp in session.get_inputs():\n",
    "        if \"pixel\" in inp.name:\n",
    "            feeds[inp.name] = arr\n",
    "        elif \"mask\" in inp.name:\n",
    "            feeds[inp.name] = mask\n",
    "        else:\n",
    "            feeds[inp.name] = ids\n",
    "\n",
    "    vec = session.run([\"image_embeds\"], feeds)[0]  # (1,512)\n",
    "    return vec[0]\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2) Load and embed image\n",
    "full_img = Image.open(\"screenshot_002.png\").convert(\"RGB\")\n",
    "jpeg_bytes = compress_image(full_img)\n",
    "vec = embed(img_session, jpeg_bytes)\n",
    "print(f\"Vector shape: {vec.shape}, dtype: {vec.dtype}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3) Classification Labels\n",
    "import json\n",
    "\n",
    "# Load labels from JSON\n",
    "with open(\"labels.json\", \"r\") as f:\n",
    "    label_data = json.load(f)\n",
    "\n",
    "APP_LABELS = label_data[\"apps\"]\n",
    "ACT_LABELS = label_data[\"actions\"]\n",
    "ACTION_PRIORS = label_data[\"action_priors\"]\n",
    "CATEGORY_LABELS = label_data[\"categories\"]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4) Text Embedding (uses same ONNX session)\n",
    "tok = CLIPTokenizerFast.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def text_embed(sentences):\n",
    "    toks = tok(sentences, padding=True, return_tensors=\"np\")\n",
    "    feeds = {\n",
    "        \"input_ids\": toks[\"input_ids\"],\n",
    "        \"attention_mask\": toks[\"attention_mask\"],\n",
    "        \"pixel_values\": np.zeros((len(sentences), 3, 224, 224), dtype=np.float32)\n",
    "    }\n",
    "    return img_session.run([\"text_embeds\"], feeds)[0]\n",
    "\n",
    "def apply_action_priors(app_name: str, act_scores: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Multiply action logits by the prior weights for the chosen app.\n",
    "    Unknown pairs default to 1.0 (no change).\n",
    "    \"\"\"\n",
    "    priors = ACTION_PRIORS.get(app_name, {})\n",
    "    boosted = act_scores.copy()\n",
    "    for act, weight in priors.items():\n",
    "        try:\n",
    "            j = ACT_LABELS.index(act)\n",
    "            boosted[j] *= weight\n",
    "        except ValueError:\n",
    "            pass                         # action not in list\n",
    "    return boosted\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 5) Build Label Embeddings\n",
    "app_vecs = text_embed(APP_LABELS)\n",
    "app_vecs /= np.linalg.norm(app_vecs, axis=1, keepdims=True)\n",
    "\n",
    "act_vecs = text_embed(ACT_LABELS)\n",
    "act_vecs /= np.linalg.norm(act_vecs, axis=1, keepdims=True)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 6) Classification Function\n",
    "def classify_with_priors(img_vec, top_k_apps=3):\n",
    "    vec = img_vec / np.linalg.norm(img_vec)\n",
    "\n",
    "    # --- raw similarity scores\n",
    "    app_scores = app_vecs @ vec               # shape (n_apps,)\n",
    "    act_scores = act_vecs @ vec               # shape (n_actions,)\n",
    "\n",
    "    # --- choose top-K app candidates\n",
    "    top_app_idx = app_scores.argsort()[-top_k_apps:][::-1]\n",
    "    top_probs   = app_scores[top_app_idx]     # still cosine, not softmax\n",
    "\n",
    "    # --- aggregate boosted action scores from each candidate app\n",
    "    agg_act_scores = np.zeros_like(act_scores)\n",
    "    for idx, score in zip(top_app_idx, top_probs):\n",
    "        app_name   = APP_LABELS[idx]\n",
    "        boosted    = apply_action_priors(app_name, act_scores)\n",
    "        agg_act_scores += score * boosted      # weight by app confidence\n",
    "\n",
    "    # --- final predictions\n",
    "    best_app_idx = top_app_idx[0]\n",
    "    best_act_idx = agg_act_scores.argmax()\n",
    "\n",
    "    return {\n",
    "        \"app\": APP_LABELS[best_app_idx],\n",
    "        \"action\": ACT_LABELS[best_act_idx],\n",
    "        \"confidence\": {\n",
    "            \"app\": float(app_scores[best_app_idx]),\n",
    "            \"action_raw\": float(act_scores[best_act_idx]),\n",
    "            \"action_boosted\": float(agg_act_scores[best_act_idx])\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 7) Run Classification\n",
    "result = classify_with_priors(vec, top_k_apps=2)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d230d",
   "metadata": {},
   "source": [
    "# Text to text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45338c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"plain_text\": \"typingstuf,doingworkandtypingstuff\\n\",\n",
      "  \"best_label\": {\n",
      "    \"label\": \"continuous_typing\",\n",
      "    \"category\": \"length-intensity\",\n",
      "    \"similarity\": 0.8902647495269775\n",
      "  },\n",
      "  \"top_k\": [\n",
      "    {\n",
      "      \"rank\": 1,\n",
      "      \"label\": \"continuous_typing\",\n",
      "      \"category\": \"length-intensity\",\n",
      "      \"similarity\": 0.8902647495269775\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 2,\n",
      "      \"label\": \"bug_report\",\n",
      "      \"category\": \"content-domain\",\n",
      "      \"similarity\": 0.8863478899002075\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 3,\n",
      "      \"label\": \"very_short_reply\",\n",
      "      \"category\": \"length-intensity\",\n",
      "      \"similarity\": 0.883529543876648\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# >>> one-time globals (reuse across calls) <<<\n",
    "\n",
    "# 1. flatten your hierarchical label dict\n",
    "flat_labels, label2cat = [], {}\n",
    "for cat, items in TEXT_LABELS.items():\n",
    "    for lbl in items:\n",
    "        flat_labels.append(lbl)\n",
    "        label2cat[lbl] = cat\n",
    "\n",
    "# 2. embed all label prompts once\n",
    "label_vecs = text_embed(flat_labels)          # shape (N,512)\n",
    "label_vecs /= np.linalg.norm(label_vecs, axis=1, keepdims=True)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# helper: reconstruct plain text from key list (you already had this)\n",
    "def reconstruct(keys: List[str]) -> str:\n",
    "    buffer = []\n",
    "    for k in keys:\n",
    "        if k in (\"Key.backspace\", \"Key.delete\"):\n",
    "            if buffer:\n",
    "                buffer.pop()\n",
    "        elif k == \"Key.enter\":\n",
    "            buffer.append(\"\\n\")\n",
    "        elif k.startswith(\"Key.\"):\n",
    "            continue\n",
    "        else:\n",
    "            buffer.append(k)\n",
    "    return \"\".join(buffer)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "def summarize_keystrokes(\n",
    "        keys: List[str],\n",
    "        top_k: int          = 5,\n",
    "        return_json: bool   = True\n",
    "    ) -> Dict:\n",
    "    \"\"\"\n",
    "    keys         : list of pynput key names\n",
    "    top_k        : number of top labels to report\n",
    "    return_json  : if True â†’ JSON string, else Python dict\n",
    "    \"\"\"\n",
    "    plain_text = reconstruct(keys)\n",
    "\n",
    "    # --- embed typed string --------------------------\n",
    "    vec = text_embed([plain_text])[0]\n",
    "    vec /= np.linalg.norm(vec)\n",
    "\n",
    "    # --- similarity scores ---------------------------\n",
    "    sims = label_vecs @ vec\n",
    "    idx  = sims.argsort()[-top_k:][::-1]\n",
    "\n",
    "    # --- build summary dict --------------------------\n",
    "    best_idx = idx[0]\n",
    "    summary = {\n",
    "        \"plain_text\": plain_text,\n",
    "        \"best_label\": {\n",
    "            \"label\":      flat_labels[best_idx],\n",
    "            \"category\":   label2cat[flat_labels[best_idx]],\n",
    "            \"similarity\": float(sims[best_idx])\n",
    "        },\n",
    "        \"top_k\": [\n",
    "            {\n",
    "                \"rank\":       rank + 1,\n",
    "                \"label\":      flat_labels[i],\n",
    "                \"category\":   label2cat[flat_labels[i]],\n",
    "                \"similarity\": float(sims[i])\n",
    "            }\n",
    "            for rank, i in enumerate(idx)\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    return json.dumps(summary, ensure_ascii=False, indent=2) if return_json else summary\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# >>> EXAMPLE USAGE <<<\n",
    "\n",
    "raw_keys = [\n",
    "    \"Key.backspace\", \"t\", \"y\", \"p\", \"i\", \"n\", \"g\", \"Key.space\",\n",
    "    \"s\", \"t\", \"u\", \"f\", \"f\", \"Key.space\", \"Key.backspace\", \",\",\n",
    "    \"Key.space\", \"d\", \"o\", \"i\", \"n\", \"g\", \"Key.space\",\n",
    "    \"w\", \"o\", \"r\", \"k\", \"Key.space\",\n",
    "    \"a\", \"n\", \"d\", \"Key.space\",\n",
    "    \"t\", \"y\", \"p\", \"i\", \"n\", \"g\", \"Key.space\",\n",
    "    \"s\", \"t\", \"u\", \"f\", \"f\", \"Key.enter\"\n",
    "]\n",
    "\n",
    "print(summarize_keystrokes(raw_keys, top_k=3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
