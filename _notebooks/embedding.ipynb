{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab67e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, transformers, os\n",
    "from utilities import embed, capture_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a69eae68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector shape: (512,), dtype: float32\n"
     ]
    }
   ],
   "source": [
    "import io, numpy as np, onnxruntime as ort\n",
    "from PIL import Image\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0) Create the ONNX session once\n",
    "img_session = ort.InferenceSession(\"tiny_clip/model.onnx\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Utilities ------------------------------------------------------\n",
    "\n",
    "def compress_image(img, quality=100):\n",
    "    import io\n",
    "    buf = io.BytesIO()\n",
    "    img.save(buf, format=\"JPEG\", quality=quality)\n",
    "    return buf.getvalue()\n",
    "\n",
    "def embed(session: ort.InferenceSession, jpeg_bytes: bytes) -> np.ndarray:\n",
    "    \"\"\"JPEG bytes --> 512-D Tiny-CLIP vector.\"\"\"\n",
    "    img = (Image.open(io.BytesIO(jpeg_bytes))\n",
    "           .convert(\"RGB\")\n",
    "           .resize((224, 224), Image.Resampling.BICUBIC))\n",
    "\n",
    "    arr = (np.asarray(img, dtype=np.float32).transpose(2, 0, 1) / 127.5) - 1.0\n",
    "    arr = arr[np.newaxis, ...]              # (1,3,224,224)\n",
    "\n",
    "    ids  = np.zeros((1, 77), dtype=np.int64)    # dummy\n",
    "    mask = np.ones((1, 77),  dtype=np.int64)    # dummy\n",
    "    feeds = {}\n",
    "    for inp in session.get_inputs():\n",
    "        if \"pixel\"  in inp.name: feeds[inp.name] = arr\n",
    "        elif \"mask\" in inp.name: feeds[inp.name] = mask\n",
    "        else:                    feeds[inp.name] = ids\n",
    "\n",
    "    vec = session.run([\"image_embeds\"], feeds)[0]   # (1,512)\n",
    "    return vec[0]                                   # (512,)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) End-to-end -----------------------------------------------------\n",
    "#full_img     = capture_full()               #  ⟵ PIL.Image\n",
    "full_img = Image.open(\"screenshot_001.jpg\").convert(\"RGB\")\n",
    "jpeg_bytes   = compress_image(full_img)     #  ⟵ raw bytes\n",
    "vec          = embed(img_session, jpeg_bytes)\n",
    "\n",
    "print(f\"Vector shape: {vec.shape}, dtype: {vec.dtype}\")\n",
    "# ► Vector shape: (512,), dtype: float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88da5491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "full_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814d0a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('code editor', 0.26631882786750793), ('OverLeaf', 0.2621487081050873), ('Microsoft Word', 0.259771466255188)]\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "import io, numpy as np, onnxruntime as ort\n",
    "from PIL import Image\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0) Create the ONNX session once\n",
    "img_session = ort.InferenceSession(\"tiny_clip/model.onnx\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Utilities ------------------------------------------------------\n",
    "\n",
    "def compress_image(img, quality=85):\n",
    "    import io\n",
    "    buf = io.BytesIO()\n",
    "    img.save(buf, format=\"JPEG\", quality=quality)\n",
    "    return buf.getvalue()\n",
    "\n",
    "def embed(session: ort.InferenceSession, jpeg_bytes: bytes) -> np.ndarray:\n",
    "    \"\"\"JPEG bytes --> 512-D Tiny-CLIP vector.\"\"\"\n",
    "    img = (Image.open(io.BytesIO(jpeg_bytes))\n",
    "           .convert(\"RGB\")\n",
    "           .resize((224, 224), Image.Resampling.BICUBIC))\n",
    "\n",
    "    arr = (np.asarray(img, dtype=np.float32).transpose(2, 0, 1) / 127.5) - 1.0\n",
    "    arr = arr[np.newaxis, ...]              # (1,3,224,224)\n",
    "\n",
    "    ids  = np.zeros((1, 77), dtype=np.int64)    # dummy\n",
    "    mask = np.ones((1, 77),  dtype=np.int64)    # dummy\n",
    "    feeds = {}\n",
    "    for inp in session.get_inputs():\n",
    "        if \"pixel\"  in inp.name: feeds[inp.name] = arr\n",
    "        elif \"mask\" in inp.name: feeds[inp.name] = mask\n",
    "        else:                    feeds[inp.name] = ids\n",
    "\n",
    "    vec = session.run([\"image_embeds\"], feeds)[0]   # (1,512)\n",
    "    return vec[0]                                   # (512,)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) End-to-end -----------------------------------------------------\n",
    "#full_img     = capture_full()               #  ⟵ PIL.Image\n",
    "full_img = Image.open(\"screenshot_001.jpg\").convert(\"RGB\")\n",
    "jpeg_bytes   = compress_image(full_img)     #  ⟵ raw bytes\n",
    "vec          = embed(img_session, jpeg_bytes)\n",
    "\n",
    "print(f\"Vector shape: {vec.shape}, dtype: {vec.dtype}\")\n",
    "# ► Vector shape: (512,), dtype: float32\n",
    "\n",
    "\n",
    "\n",
    "LABELS = [\"login screen\", \"error dialog\", \"code editor\",\n",
    "          \"settings page\", \"spreadsheet\", \"browser home page\", \"vscode\", \"Microsoft Word\", \"OverLeaf\", \"writing a paper\"]\n",
    "\n",
    "# Make a *text* embedding table once (reuse the SAME session)\n",
    "from transformers import CLIPTokenizerFast\n",
    "tok = CLIPTokenizerFast.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def text_embed(sentences):\n",
    "    toks = tok(sentences, padding=True, return_tensors=\"np\")\n",
    "    feeds = {\n",
    "        \"input_ids\":      toks[\"input_ids\"],\n",
    "        \"attention_mask\": toks[\"attention_mask\"],\n",
    "        # dummy image input so ONNX doesn’t complain\n",
    "        \"pixel_values\":   np.zeros((len(sentences), 3, 224, 224), np.float32)\n",
    "    }\n",
    "    return img_session.run([\"text_embeds\"], feeds)[0]\n",
    "\n",
    "label_vecs = text_embed(LABELS)\n",
    "label_vecs /= np.linalg.norm(label_vecs, axis=1, keepdims=True)\n",
    "\n",
    "def describe(vec, top_k=3):\n",
    "    vec  = vec / np.linalg.norm(vec)\n",
    "    sims = label_vecs @ vec\n",
    "    idx  = sims.argsort()[-top_k:][::-1]\n",
    "    return [(LABELS[i], float(sims[i])) for i in idx]\n",
    "\n",
    "print(describe(vec))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
